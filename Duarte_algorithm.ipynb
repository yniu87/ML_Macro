{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We solve the Neoclassical Growth Model\n",
    "\\begin{align}\n",
    "  \\rho V(k) &= \\max_{c} u(c) +  V'(k)(F(k)-\\delta k - c) \\\\\n",
    "  &u(c) = \\frac{c^{1-\\sigma}}{1-\\sigma},~~~~ F(k)=k^\\alpha\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 2 # 1 changes to log utility\n",
    "alpha = 0.3\n",
    "delta = 0.05\n",
    "rho = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kss = (alpha/(rho+delta))**(1/(1-alpha))\n",
    "kmin = 0.001*kss\n",
    "kmax = 2*kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = 50000\n",
    "train = np.random.uniform(kmin,kmax,(train_num,1))\n",
    "# train = np.linspace(kmin, kmax, num=train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(2, activation='relu', input_shape=[1]), #use_bias=True\n",
    "    layers.Dense(4, activation='relu'), #use_bias=False     \n",
    "    layers.Dense(8, activation='relu'), #use_bias=False\n",
    "    layers.Dense(16, activation='relu'), #use_bias=False     \n",
    "    layers.Dense(8, activation='relu'), #use_bias=False \n",
    "    layers.Dense(4, activation='relu'), #use_bias=False \n",
    "    layers.Dense(2, activation='relu'), #use_bias=False   \n",
    "    layers.Dense(1) #,kernel_constraint=keras.constraints.NonNeg()\n",
    "  ])\n",
    "\n",
    "  optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#   optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                )\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 2)                 4         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 385\n",
      "Trainable params: 385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# model.get_weights() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "40000/40000 [==============================] - 1s 13us/sample - loss: 373.9791 - val_loss: 369.4063\n",
      "Epoch 2/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 362.1366 - val_loss: 349.2700\n",
      "Epoch 3/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 318.6531 - val_loss: 267.9897\n",
      "Epoch 4/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 184.5324 - val_loss: 96.8009\n",
      "Epoch 5/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 78.9855 - val_loss: 71.7333\n",
      "Epoch 6/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 67.8249 - val_loss: 62.5708\n",
      "Epoch 7/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 58.7439 - val_loss: 53.7314\n",
      "Epoch 8/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 49.7815 - val_loss: 44.8225\n",
      "Epoch 9/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 41.2348 - val_loss: 36.9005\n",
      "Epoch 10/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 33.7551 - val_loss: 30.0969\n",
      "Epoch 11/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 27.5011 - val_loss: 24.5202\n",
      "Epoch 12/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 22.4578 - val_loss: 20.0707\n",
      "Epoch 13/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 18.4749 - val_loss: 16.6250\n",
      "Epoch 14/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 15.3906 - val_loss: 13.9346\n",
      "Epoch 15/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 12.9726 - val_loss: 11.8221\n",
      "Epoch 16/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 11.0585 - val_loss: 10.1196\n",
      "Epoch 17/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 9.4848 - val_loss: 8.6968\n",
      "Epoch 18/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 8.1460 - val_loss: 7.4677\n",
      "Epoch 19/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 6.9556 - val_loss: 6.3384\n",
      "Epoch 20/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 5.8554 - val_loss: 5.2626\n",
      "Epoch 21/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.8060 - val_loss: 4.2450\n",
      "Epoch 22/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.7888 - val_loss: 3.2590\n",
      "Epoch 23/200\n",
      "40000/40000 [==============================] - 0s 5us/sample - loss: 2.8271 - val_loss: 2.3445\n",
      "Epoch 24/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 1.9532 - val_loss: 1.5382\n",
      "Epoch 25/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 1.2212 - val_loss: 0.9004\n",
      "Epoch 26/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 0.6761 - val_loss: 0.4635\n",
      "Epoch 27/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 0.3285 - val_loss: 0.2098\n",
      "Epoch 28/200\n",
      "40000/40000 [==============================] - 0s 5us/sample - loss: 0.1409 - val_loss: 0.0757\n",
      "Epoch 29/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 0.0398 - val_loss: 0.0178\n",
      "Epoch 30/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 0.0101 - val_loss: 0.0053\n",
      "Epoch 31/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 0.0035 - val_loss: 0.0023\n",
      "Epoch 32/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 33/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 9.0824e-04 - val_loss: 6.8858e-04\n",
      "Epoch 34/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 5.4575e-04 - val_loss: 4.3300e-04\n",
      "Epoch 35/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 3.5020e-04 - val_loss: 2.8635e-04\n",
      "Epoch 36/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.3547e-04 - val_loss: 2.0062e-04\n",
      "Epoch 37/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 1.6650e-04 - val_loss: 1.4256e-04\n",
      "Epoch 38/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 1.2042e-04 - val_loss: 1.0618e-04\n",
      "Epoch 39/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 8.9631e-05 - val_loss: 7.9963e-05\n",
      "Epoch 40/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 6.7854e-05 - val_loss: 6.1574e-05\n",
      "Epoch 41/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 5.2448e-05 - val_loss: 4.8052e-05\n",
      "Epoch 42/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.1020e-05 - val_loss: 3.8117e-05\n",
      "Epoch 43/200\n",
      "40000/40000 [==============================] - 0s 5us/sample - loss: 3.2533e-05 - val_loss: 3.0401e-05\n",
      "Epoch 44/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.6134e-05 - val_loss: 2.4462e-05\n",
      "Epoch 45/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.1160e-05 - val_loss: 2.0320e-05\n",
      "Epoch 46/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.7374e-05 - val_loss: 1.6702e-05\n",
      "Epoch 47/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.4365e-05 - val_loss: 1.3884e-05\n",
      "Epoch 48/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.1958e-05 - val_loss: 1.1687e-05\n",
      "Epoch 49/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 1.0035e-05 - val_loss: 9.9438e-06\n",
      "Epoch 50/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 8.4982e-06 - val_loss: 8.3874e-06\n",
      "Epoch 51/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 7.2214e-06 - val_loss: 7.1649e-06\n",
      "Epoch 52/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 6.1349e-06 - val_loss: 6.1838e-06\n",
      "Epoch 53/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 5.2738e-06 - val_loss: 5.3032e-06\n",
      "Epoch 54/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 4.5278e-06 - val_loss: 4.5479e-06\n",
      "Epoch 55/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 3.9062e-06 - val_loss: 3.9322e-06\n",
      "Epoch 56/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.3755e-06 - val_loss: 3.4178e-06\n",
      "Epoch 57/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.9307e-06 - val_loss: 2.9551e-06\n",
      "Epoch 58/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.5455e-06 - val_loss: 2.5722e-06\n",
      "Epoch 59/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 2.2159e-06 - val_loss: 2.2539e-06\n",
      "Epoch 60/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.9385e-06 - val_loss: 1.9664e-06\n",
      "Epoch 61/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 1.6967e-06 - val_loss: 1.7172e-06\n",
      "Epoch 62/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 1.4908e-06 - val_loss: 1.5063e-06\n",
      "Epoch 63/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.3104e-06 - val_loss: 1.3189e-06\n",
      "Epoch 64/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.1506e-06 - val_loss: 1.1743e-06\n",
      "Epoch 65/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 1.0250e-06 - val_loss: 1.0329e-06\n",
      "Epoch 66/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 9.0363e-07 - val_loss: 9.1214e-07\n",
      "Epoch 67/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 7.9997e-07 - val_loss: 8.0075e-07\n",
      "Epoch 68/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 7.1019e-07 - val_loss: 7.0876e-07\n",
      "Epoch 69/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 6.3408e-07 - val_loss: 6.2966e-07\n",
      "Epoch 70/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 5.6610e-07 - val_loss: 5.5926e-07\n",
      "Epoch 71/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 5.0757e-07 - val_loss: 5.0031e-07\n",
      "Epoch 72/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 4.5522e-07 - val_loss: 4.4657e-07\n",
      "Epoch 73/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.1095e-07 - val_loss: 3.9953e-07\n",
      "Epoch 74/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.7039e-07 - val_loss: 3.5865e-07\n",
      "Epoch 75/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 3.3634e-07 - val_loss: 3.2152e-07\n",
      "Epoch 76/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 3.0454e-07 - val_loss: 2.8970e-07\n",
      "Epoch 77/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.7604e-07 - val_loss: 2.6294e-07\n",
      "Epoch 78/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.5241e-07 - val_loss: 2.3529e-07\n",
      "Epoch 79/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.2983e-07 - val_loss: 2.1199e-07\n",
      "Epoch 80/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.0947e-07 - val_loss: 1.9261e-07\n",
      "Epoch 81/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.9251e-07 - val_loss: 1.7341e-07\n",
      "Epoch 82/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.7539e-07 - val_loss: 1.6250e-07\n",
      "Epoch 83/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.6345e-07 - val_loss: 1.4484e-07\n",
      "Epoch 84/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.4846e-07 - val_loss: 1.3059e-07\n",
      "Epoch 85/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.3625e-07 - val_loss: 1.2095e-07\n",
      "Epoch 86/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.2589e-07 - val_loss: 1.0931e-07\n",
      "Epoch 87/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.1542e-07 - val_loss: 9.9933e-08\n",
      "Epoch 88/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.0722e-07 - val_loss: 9.2152e-08\n",
      "Epoch 89/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 9.9908e-08 - val_loss: 8.2604e-08\n",
      "Epoch 90/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 9.1705e-08 - val_loss: 7.5811e-08\n",
      "Epoch 91/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 8.5060e-08 - val_loss: 6.9613e-08\n",
      "Epoch 92/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 7.8841e-08 - val_loss: 6.4422e-08\n",
      "Epoch 93/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 7.3633e-08 - val_loss: 5.9020e-08\n",
      "Epoch 94/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 6.8824e-08 - val_loss: 5.4584e-08\n",
      "Epoch 95/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 6.3905e-08 - val_loss: 4.9223e-08\n",
      "Epoch 96/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 5.8937e-08 - val_loss: 4.7170e-08\n",
      "Epoch 97/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 5.5257e-08 - val_loss: 4.1344e-08\n",
      "Epoch 98/200\n",
      "40000/40000 [==============================] - 0s 5us/sample - loss: 5.1334e-08 - val_loss: 3.8094e-08\n",
      "Epoch 99/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 4.7996e-08 - val_loss: 3.5381e-08\n",
      "Epoch 100/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.4507e-08 - val_loss: 3.2478e-08\n",
      "Epoch 101/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.1651e-08 - val_loss: 2.9884e-08\n",
      "Epoch 102/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.8687e-08 - val_loss: 2.7127e-08\n",
      "Epoch 103/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 3.6448e-08 - val_loss: 2.4884e-08\n",
      "Epoch 104/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.4044e-08 - val_loss: 2.2520e-08\n",
      "Epoch 105/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 3.1481e-08 - val_loss: 2.0825e-08\n",
      "Epoch 106/200\n",
      "40000/40000 [==============================] - 0s 5us/sample - loss: 2.9447e-08 - val_loss: 1.8996e-08\n",
      "Epoch 107/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.7621e-08 - val_loss: 1.7063e-08\n",
      "Epoch 108/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.5597e-08 - val_loss: 1.5603e-08\n",
      "Epoch 109/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.4393e-08 - val_loss: 1.4255e-08\n",
      "Epoch 110/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 2.2620e-08 - val_loss: 1.2743e-08\n",
      "Epoch 111/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 2.0794e-08 - val_loss: 1.1689e-08\n",
      "Epoch 112/200\n",
      "40000/40000 [==============================] - 0s 5us/sample - loss: 1.9305e-08 - val_loss: 1.0756e-08\n",
      "Epoch 113/200\n",
      "40000/40000 [==============================] - 0s 5us/sample - loss: 1.8177e-08 - val_loss: 9.4177e-09\n",
      "Epoch 114/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.7033e-08 - val_loss: 8.9365e-09\n",
      "Epoch 115/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 1.5836e-08 - val_loss: 7.6477e-09\n",
      "Epoch 116/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.4809e-08 - val_loss: 7.3590e-09\n",
      "Epoch 117/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 1.3828e-08 - val_loss: 6.2013e-09\n",
      "Epoch 118/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 1.2885e-08 - val_loss: 5.8980e-09\n",
      "Epoch 119/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 1.2001e-08 - val_loss: 5.0327e-09\n",
      "Epoch 120/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.1233e-08 - val_loss: 4.7315e-09\n",
      "Epoch 121/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.0488e-08 - val_loss: 4.0109e-09\n",
      "Epoch 122/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 9.7294e-09 - val_loss: 3.6587e-09\n",
      "Epoch 123/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 9.1277e-09 - val_loss: 3.9755e-09\n",
      "Epoch 124/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 8.6224e-09 - val_loss: 2.9524e-09\n",
      "Epoch 125/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 7.9833e-09 - val_loss: 2.9483e-09\n",
      "Epoch 126/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 7.5950e-09 - val_loss: 2.7665e-09\n",
      "Epoch 127/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 6.9971e-09 - val_loss: 2.2040e-09\n",
      "Epoch 128/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 6.6714e-09 - val_loss: 2.0997e-09\n",
      "Epoch 129/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 6.0671e-09 - val_loss: 1.7920e-09\n",
      "Epoch 130/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 5.6735e-09 - val_loss: 1.7408e-09\n",
      "Epoch 131/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 5.4038e-09 - val_loss: 1.5240e-09\n",
      "Epoch 132/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 5.0017e-09 - val_loss: 1.3646e-09\n",
      "Epoch 133/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.6885e-09 - val_loss: 1.6075e-09\n",
      "Epoch 134/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.4743e-09 - val_loss: 1.1780e-09\n",
      "Epoch 135/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.0651e-09 - val_loss: 1.0197e-09\n",
      "Epoch 136/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 3.7420e-09 - val_loss: 9.7789e-10\n",
      "Epoch 137/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.4227e-09 - val_loss: 1.0434e-09\n",
      "Epoch 138/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.6050e-09 - val_loss: 1.0270e-09\n",
      "Epoch 139/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.1509e-09 - val_loss: 6.9329e-10\n",
      "Epoch 140/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.8253e-09 - val_loss: 6.8375e-10\n",
      "Epoch 141/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.5373e-09 - val_loss: 5.6917e-10\n",
      "Epoch 142/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.5088e-09 - val_loss: 7.5440e-10\n",
      "Epoch 143/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.2855e-09 - val_loss: 5.5861e-10\n",
      "Epoch 144/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.0642e-09 - val_loss: 6.9802e-10\n",
      "Epoch 145/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.9688e-09 - val_loss: 4.1487e-10\n",
      "Epoch 146/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.8217e-09 - val_loss: 3.4336e-10\n",
      "Epoch 147/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.6852e-09 - val_loss: 3.2714e-10\n",
      "Epoch 148/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.6275e-09 - val_loss: 2.8937e-10\n",
      "Epoch 149/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.7867e-09 - val_loss: 8.0864e-10\n",
      "Epoch 150/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.5201e-09 - val_loss: 1.3461e-09\n",
      "Epoch 151/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 1.6264e-08 - val_loss: 3.9578e-08\n",
      "Epoch 152/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.9040e-06 - val_loss: 9.9871e-08\n",
      "Epoch 153/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.3140e-07 - val_loss: 5.5056e-08\n",
      "Epoch 154/200\n",
      "40000/40000 [==============================] - 0s 4us/sample - loss: 1.5539e-06 - val_loss: 1.7109e-05\n",
      "Epoch 155/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.1317e-06 - val_loss: 6.2137e-07\n",
      "Epoch 156/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 8.7871e-08 - val_loss: 8.5804e-09\n",
      "Epoch 157/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.6657e-07 - val_loss: 6.3653e-06\n",
      "Epoch 158/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 9.4831e-06 - val_loss: 1.2072e-07\n",
      "Epoch 159/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.2432e-07 - val_loss: 1.6818e-08\n",
      "Epoch 160/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 6.4107e-09 - val_loss: 2.5694e-08\n",
      "Epoch 161/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 2.2244e-05 - val_loss: 1.6226e-05\n",
      "Epoch 162/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.9114e-06 - val_loss: 3.2417e-08\n",
      "Epoch 163/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 3.1555e-08 - val_loss: 3.2620e-09\n",
      "Epoch 164/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 1.9169e-07 - val_loss: 2.7956e-06\n",
      "Epoch 165/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.4429e-05 - val_loss: 1.1878e-05\n",
      "Epoch 166/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 1.6256e-06 - val_loss: 3.0402e-07\n",
      "Epoch 167/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.6620e-08 - val_loss: 1.1858e-08\n",
      "Epoch 168/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 6.5832e-05 - val_loss: 1.2563e-04\n",
      "Epoch 169/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 2.0770e-05 - val_loss: 7.6677e-07\n",
      "Epoch 170/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 3.7698e-07 - val_loss: 1.8399e-08\n",
      "Epoch 171/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 8.2604e-08 - val_loss: 5.5724e-07\n",
      "Epoch 172/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.3584e-04 - val_loss: 7.1645e-06\n",
      "Epoch 173/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 9.6094e-06 - val_loss: 3.0375e-07\n",
      "Epoch 174/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.2587e-07 - val_loss: 1.8291e-08\n",
      "Epoch 175/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 5.2750e-08 - val_loss: 9.7380e-08\n",
      "Epoch 176/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.5598e-04 - val_loss: 3.4274e-04\n",
      "Epoch 177/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.2677e-05 - val_loss: 2.1805e-06\n",
      "Epoch 178/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 7.7918e-07 - val_loss: 3.9785e-08\n",
      "Epoch 179/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 7.7142e-08 - val_loss: 9.5214e-09\n",
      "Epoch 180/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 3.2461e-06 - val_loss: 5.4864e-05\n",
      "Epoch 181/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 1.9727e-04 - val_loss: 2.9226e-05\n",
      "Epoch 182/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 4.8483e-06 - val_loss: 1.9380e-07\n",
      "Epoch 183/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 2.3011e-07 - val_loss: 1.9879e-07\n",
      "Epoch 184/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.7419e-06 - val_loss: 2.6195e-05\n",
      "Epoch 185/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 2.1159e-04 - val_loss: 4.1442e-08\n",
      "Epoch 186/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 5.4863e-06 - val_loss: 8.1607e-07\n",
      "Epoch 187/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 1.7437e-07 - val_loss: 1.6480e-08\n",
      "Epoch 188/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 3.9865e-06 - val_loss: 7.2375e-05\n",
      "Epoch 189/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 2.5608e-04 - val_loss: 3.7294e-05\n",
      "Epoch 190/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 5.6920e-06 - val_loss: 7.9236e-07\n",
      "Epoch 191/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.3981e-07 - val_loss: 1.4874e-08\n",
      "Epoch 192/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 9.8760e-08 - val_loss: 4.8227e-07\n",
      "Epoch 193/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 3.0522e-04 - val_loss: 9.6297e-05\n",
      "Epoch 194/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 2.5002e-05 - val_loss: 3.6403e-06\n",
      "Epoch 195/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 6.2255e-07 - val_loss: 3.0739e-08\n",
      "Epoch 196/200\n",
      "40000/40000 [==============================] - 0s 2us/sample - loss: 6.2909e-08 - val_loss: 1.8299e-08\n",
      "Epoch 197/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.4151e-05 - val_loss: 3.7994e-04\n",
      "Epoch 198/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.3223e-04 - val_loss: 2.8257e-05\n",
      "Epoch 199/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 4.0162e-06 - val_loss: 1.1569e-06\n",
      "Epoch 200/200\n",
      "40000/40000 [==============================] - 0s 3us/sample - loss: 2.8159e-07 - val_loss: 8.3918e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11af5b860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_guess = -24+ train\n",
    "model.fit( train, initial_guess, epochs=200, batch_size=1000,  validation_split = 0.2, verbose=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2042337782451313"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.linspace(kmin,kmax,num=1000)\n",
    "\n",
    "y_test = -24+ x_test\n",
    "\n",
    "error = y_test - model.predict([x_test])\n",
    "\n",
    "np.mean(abs(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99999374]\n",
      " [0.99999374]\n",
      " [0.99999374]\n",
      " ...\n",
      " [0.99999374]\n",
      " [0.99999374]\n",
      " [0.99999374]]\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.constant(train)\n",
    "with tf.GradientTape() as t: #(persistent=True)\n",
    "    t.watch(inputs)\n",
    "    output = model(inputs)\n",
    "derivative = t.gradient(output, inputs).numpy()\n",
    "print(derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.ones(train_num).reshape(train_num,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "update = 10\n",
    "dt = 5*1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0.99999374]\n",
      " [0.99999374]\n",
      " [0.99999374]\n",
      " ...\n",
      " [0.99999374]\n",
      " [0.99999374]\n",
      " [0.99999374]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[1.0011853]\n",
      " [1.0011853]\n",
      " [1.0011853]\n",
      " ...\n",
      " [1.0011853]\n",
      " [1.0011853]\n",
      " [1.0011853]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[1.00577521]\n",
      " [0.99987453]\n",
      " [1.00577521]\n",
      " ...\n",
      " [0.99987453]\n",
      " [0.99987453]\n",
      " [0.99987453]]\n"
     ]
    }
   ],
   "source": [
    "for update in range(update):\n",
    "        value = model.predict([train])\n",
    "        print(type(value))\n",
    "\n",
    "        with tf.GradientTape() as t: #(persistent=True)\n",
    "            t.watch(inputs)\n",
    "            output = model(inputs)\n",
    "            \n",
    "        derivative = t.gradient(output, inputs).numpy()\n",
    "        print(derivative)\n",
    "\n",
    "        for i in range(train_num):\n",
    "            derivative[i] = np.maximum(derivative[i], 1e-4)\n",
    "            \n",
    "        # with log utility function\n",
    "#         HJB = np.log(1/derivative)+derivative*((train)**alpha-delta*train-1/derivative)-rho*value\n",
    "\n",
    "        # with sigma=2 CRRA utility function\n",
    "        HJB = (1/np.sqrt(derivative))**(1-sigma)/(1-sigma)+derivative*((train)**alpha-delta*train-1/np.sqrt(derivative))-rho*value\n",
    "#         print(HJB)\n",
    "        value += dt*HJB\n",
    "        model.fit( train, value,  epochs=2000, batch_size=2000, validation_split = 0.2, verbose=0 )\n",
    "#         print(update,  max(abs(HJB)), np.mean(abs(HJB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(kmin, kmax, num=100)\n",
    "y = model.predict([x])\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,y)\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('V(k)')\n",
    "plt.title('Value function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
